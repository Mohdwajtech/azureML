{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups=fetch_20newsgroups()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n",
    "remove = ('headers', 'footers', 'quotes')\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', categories=categories, remove=remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi,\n",
      "\n",
      "I've noticed that if you only save a model (with all your mapping planes\n",
      "positioned carefully) to a .3DS file that when you reload it after restarting\n",
      "3DS, they are given a default position and orientation.  But if you save\n",
      "to a .PRJ file their positions/orientation are preserved.  Does anyone\n",
      "know why this information is not stored in the .3DS file?  Nothing is\n",
      "explicitly said in the manual about saving texture rules in the .PRJ file. \n",
      "I'd like to be able to read the texture rule information, does anyone have \n",
      "the format for the .PRJ file?\n",
      "\n",
      "Is the .CEL file format available from somewhere?\n",
      "\n",
      "Rych----------------------------\n",
      "\n",
      "\n",
      "Seems to be, barring evidence to the contrary, that Koresh was simply\n",
      "another deranged fanatic who thought it neccessary to take a whole bunch of\n",
      "folks with him, children and all, to satisfy his delusional mania. Jim\n",
      "Jones, circa 1993.\n",
      "\n",
      "\n",
      "Nope - fruitcakes like Koresh have been demonstrating such evil corruption\n",
      "for centuries.----------------------------\n",
      "\n",
      " >In article <1993Apr19.020359.26996@sq.sq.com>, msb@sq.sq.com (Mark Brader) \n",
      "\n",
      "MB>                                                             So the\n",
      "MB> 1970 figure seems unlikely to actually be anything but a perijove.\n",
      "\n",
      "JG>Sorry, _perijoves_...I'm not used to talking this language.\n",
      "\n",
      "Couldn't we just say periapsis or apoapsis?\n",
      "\n",
      " \n"
     ]
    }
   ],
   "source": [
    "print(\"----------------------------\\n\".join(newsgroups_train.data[:3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['comp.graphics' 'talk.religion.misc' 'sci.space']\n"
     ]
    }
   ],
   "source": [
    "print(np.array(newsgroups_train.target_names)[newsgroups_train.target[:3]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Split function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Seems',\n",
       " 'to',\n",
       " 'be,',\n",
       " 'barring',\n",
       " 'evidence',\n",
       " 'to',\n",
       " 'the',\n",
       " 'contrary,',\n",
       " 'that',\n",
       " 'Koresh']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_train.data[1].split()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Seems', 'to', 'be', ',', 'barring', 'evidence', 'to', 'the', 'contrary', ',']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.word_tokenize(newsgroups_train.data[1])[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@', 'john', 'lol', 'that', 'was', '#', 'awesome', ':', ')']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mytweet = \"@john lol that was #awesome :)\"\n",
    "nltk.word_tokenize(mytweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#Although this behavior might be desirable in some cases, it’s most likely that we’d prefer for @ and john to be tokenized together as @john, \n",
    "#and # and awesome to be tokenized together as #awesome. \n",
    "#This is because we’d expect that word usage in the context of hastags or at-mentions is likely different from usage in plain text. \n",
    "#Moreover, we would prefer that : and ) to be tokenized together as :), as :) is certainly more informative (e.g. for sentiment analysis) than the sum of its parts.\n",
    "\n",
    "# For custom tokenizations we use Regular Expressions which we will visit later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install spacy\n",
    "# ! python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n\\n', 'Seems', 'to', 'be', ',', 'barring', 'evidence', 'to', 'the', 'contrary']\n"
     ]
    }
   ],
   "source": [
    "tokens=[]\n",
    "doc = nlp(newsgroups_train.data[1])\n",
    "for token in doc:\n",
    "    tokens.append(token.text)\n",
    "print(tokens[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nSeems to be, barring evidence to the contrary, that Koresh was simply\\nanother deranged fanatic who thought it neccessary to take a whole bunch of\\nfolks with him, children and all, to satisfy his delusional mania. Jim\\nJones, circa 1993.\\n\\n\\nNope - fruitcakes like Koresh have been demonstrating such evil corruption\\nfor centuries.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_train.data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n\\nSeems to be, barring evidence to the contrary, that Koresh was simply\\nanother deranged fanatic who thought it neccessary to take a whole bunch of\\nfolks with him, children and all, to satisfy his delusional mania.',\n",
       " 'Jim\\nJones, circa 1993.',\n",
       " 'Nope - fruitcakes like Koresh have been demonstrating such evil corruption\\nfor centuries.']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(newsgroups_train.data[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n\\nSeems to be, barring evidence to the contrary, that Koresh was simply\\nanother deranged fanatic who thought it neccessary to take a whole bunch of\\nfolks with him, children and all, to satisfy his delusional mania.',\n",
       " 'Jim\\nJones, circa 1993.\\n\\n\\n',\n",
       " 'Nope - fruitcakes like Koresh have been demonstrating such evil corruption\\nfor centuries.']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence=[]\n",
    "doc = nlp(newsgroups_train.data[1])\n",
    "for sent in doc.sents:\n",
    "    sentence.append(sent.text)\n",
    "sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/azureuser/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Seems', 'NNS'), ('to', 'TO'), ('be,', 'VB'), ('barring', 'VBG'), ('evidence', 'NN'), ('to', 'TO'), ('the', 'DT'), ('contrary,', 'NN'), ('that', 'IN'), ('Koresh', 'NNP'), ('was', 'VBD'), ('simply', 'RB'), ('another', 'DT'), ('deranged', 'VBN'), ('fanatic', 'NN'), ('who', 'WP'), ('thought', 'VBD'), ('it', 'PRP'), ('neccessary', 'JJ'), ('to', 'TO'), ('take', 'VB'), ('a', 'DT'), ('whole', 'JJ'), ('bunch', 'NN'), ('of', 'IN'), ('folks', 'NNS'), ('with', 'IN'), ('him,', 'JJ'), ('children', 'NNS'), ('and', 'CC'), ('all,', 'NN'), ('to', 'TO'), ('satisfy', 'VB'), ('his', 'PRP$'), ('delusional', 'JJ'), ('mania.', 'NN'), ('Jim', 'NNP'), ('Jones,', 'NNP'), ('circa', 'VBD'), ('1993.', 'CD'), ('Nope', 'NNP'), ('-', ':'), ('fruitcakes', 'NNS'), ('like', 'IN'), ('Koresh', 'NNP'), ('have', 'VBP'), ('been', 'VBN'), ('demonstrating', 'VBG'), ('such', 'JJ'), ('evil', 'JJ'), ('corruption', 'NN'), ('for', 'IN'), ('centuries.', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "tokens_tag = pos_tag(newsgroups_train.data[1].split())\n",
    "print(tokens_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('\\n\\n', 'SPACE'),\n",
       " ('Seems', 'VERB'),\n",
       " ('to', 'PART'),\n",
       " ('be', 'AUX'),\n",
       " (',', 'PUNCT'),\n",
       " ('barring', 'VERB'),\n",
       " ('evidence', 'NOUN'),\n",
       " ('to', 'ADP'),\n",
       " ('the', 'DET'),\n",
       " ('contrary', 'NOUN'),\n",
       " (',', 'PUNCT'),\n",
       " ('that', 'SCONJ'),\n",
       " ('Koresh', 'PROPN'),\n",
       " ('was', 'AUX'),\n",
       " ('simply', 'ADV'),\n",
       " ('\\n', 'SPACE'),\n",
       " ('another', 'DET'),\n",
       " ('deranged', 'ADJ'),\n",
       " ('fanatic', 'NOUN'),\n",
       " ('who', 'PRON'),\n",
       " ('thought', 'VERB'),\n",
       " ('it', 'PRON'),\n",
       " ('neccessary', 'ADJ'),\n",
       " ('to', 'PART'),\n",
       " ('take', 'VERB'),\n",
       " ('a', 'DET'),\n",
       " ('whole', 'ADJ'),\n",
       " ('bunch', 'NOUN'),\n",
       " ('of', 'ADP'),\n",
       " ('\\n', 'SPACE'),\n",
       " ('folks', 'NOUN'),\n",
       " ('with', 'ADP'),\n",
       " ('him', 'PRON'),\n",
       " (',', 'PUNCT'),\n",
       " ('children', 'NOUN'),\n",
       " ('and', 'CCONJ'),\n",
       " ('all', 'DET'),\n",
       " (',', 'PUNCT'),\n",
       " ('to', 'PART'),\n",
       " ('satisfy', 'VERB'),\n",
       " ('his', 'DET'),\n",
       " ('delusional', 'ADJ'),\n",
       " ('mania', 'NOUN'),\n",
       " ('.', 'PUNCT'),\n",
       " ('Jim', 'PROPN'),\n",
       " ('\\n', 'SPACE'),\n",
       " ('Jones', 'PROPN'),\n",
       " (',', 'PUNCT'),\n",
       " ('circa', 'NOUN'),\n",
       " ('1993', 'NUM'),\n",
       " ('.', 'PUNCT'),\n",
       " ('\\n\\n\\n', 'SPACE'),\n",
       " ('Nope', 'PROPN'),\n",
       " ('-', 'PUNCT'),\n",
       " ('fruitcakes', 'NOUN'),\n",
       " ('like', 'SCONJ'),\n",
       " ('Koresh', 'PROPN'),\n",
       " ('have', 'AUX'),\n",
       " ('been', 'AUX'),\n",
       " ('demonstrating', 'VERB'),\n",
       " ('such', 'ADJ'),\n",
       " ('evil', 'ADJ'),\n",
       " ('corruption', 'NOUN'),\n",
       " ('\\n', 'SPACE'),\n",
       " ('for', 'ADP'),\n",
       " ('centuries', 'NOUN'),\n",
       " ('.', 'PUNCT')]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tags=[]\n",
    "doc = nlp(newsgroups_train.data[1])\n",
    "for token in doc:\n",
    "    pos_tags.append((token.text,token.pos_))\n",
    "pos_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Seems', 'seem'),\n",
       " ('to', 'to'),\n",
       " ('be', 'be'),\n",
       " (',', ','),\n",
       " ('barring', 'bar'),\n",
       " ('evidence', 'evid'),\n",
       " ('to', 'to'),\n",
       " ('the', 'the'),\n",
       " ('contrary', 'contrari'),\n",
       " (',', ',')]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmed=[]\n",
    "for w in nltk.word_tokenize(newsgroups_train.data[1])[:10]:\n",
    "    rootWord=ps.stem(w)\n",
    "    stemmed.append((w,rootWord))\n",
    "stemmed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/azureuser/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Seems', 'Seems'),\n",
       " ('to', 'to'),\n",
       " ('be', 'be'),\n",
       " (',', ','),\n",
       " ('barring', 'barring'),\n",
       " ('evidence', 'evidence'),\n",
       " ('to', 'to'),\n",
       " ('the', 'the'),\n",
       " ('contrary', 'contrary'),\n",
       " (',', ',')]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatized_nltk=[]\n",
    "for w in nltk.word_tokenize(newsgroups_train.data[1])[:10]:\n",
    "    rootWord=wordnet_lemmatizer.lemmatize(w)\n",
    "    lemmatized_nltk.append((w,rootWord))\n",
    "lemmatized_nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('\\n\\n', '\\n\\n'),\n",
       " ('Seems', 'seem'),\n",
       " ('to', 'to'),\n",
       " ('be', 'be'),\n",
       " (',', ','),\n",
       " ('barring', 'bar'),\n",
       " ('evidence', 'evidence'),\n",
       " ('to', 'to'),\n",
       " ('the', 'the'),\n",
       " ('contrary', 'contrary'),\n",
       " (',', ','),\n",
       " ('t', 't')]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatized=[]\n",
    "doc = nlp(newsgroups_train.data[1][:50])\n",
    "for token in doc:\n",
    "    lemmatized.append((token.text,token.lemma_))\n",
    "lemmatized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_list=[]\n",
    "doc = nlp(newsgroups_train.data[1][:50])\n",
    "for token in doc:\n",
    "    token_list.append(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n\\n', ',', 'barring', 'evidence', 'contrary', ',', 't']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_sentence =[]\n",
    "for word in token_list:\n",
    "    lexeme = nlp.vocab[word]\n",
    "    if lexeme.is_stop == False:\n",
    "        filtered_sentence.append(word) \n",
    "filtered_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_20newsgroups.html#sklearn.datasets.fetch_20newsgroups\n",
    "\n",
    "https://drive.google.com/file/d/1pWesiV90JdgqxpfkwRfuaf_PSM53-4hZ/view\n",
    "\n",
    "https://github.com/hb20007/hands-on-nltk-tutorial/blob/master/1-2-Text-Analysis-Using-nltk.text.ipynb\n",
    "\n",
    "https://github.com/hb20007/hands-on-nltk-tutorial/blob/master/3-4-Parts-of-Speech-and-Meaning.ipynb\n",
    "\n",
    "https://spacy.io/usage/spacy-101\n",
    "\n",
    "https://www.nltk.org/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
